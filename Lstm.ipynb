{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:07.704745Z",
     "start_time": "2023-12-01T11:15:07.679800Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "#print(os.listdir(\"../input\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:08.262294Z",
     "start_time": "2023-12-01T11:15:07.706403Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333333, 2)\n",
      "(333333, 2)\n"
     ]
    }
   ],
   "source": [
    "# File loading\n",
    "df  = pd.read_csv('./unigram_freq.csv/unigram_freq.csv')\n",
    "print(df.shape)\n",
    "df.dropna(axis=0,how='any')\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:08.489672Z",
     "start_time": "2023-12-01T11:15:08.266000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line Count: 333331\n",
      "['the', 'of', 'and', 'to']\n"
     ]
    }
   ],
   "source": [
    "lines = [x for x in df['word'] if type(x) == type('a') ]\n",
    "print(\"Line Count:\",len(lines))\n",
    "print(lines[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:08.517774Z",
     "start_time": "2023-12-01T11:15:08.498131Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "import re\n",
    "def process(sent):\n",
    "    sent=sent.lower()\n",
    "    sent=re.sub(r'[^0-9a-zA-Z ]','',sent)\n",
    "    sent=sent.replace('\\n','')\n",
    "    return sent    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:10.288952Z",
     "start_time": "2023-12-01T11:15:08.517774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lauritzen\n",
      "crevice\n",
      "libcap\n",
      "rocas\n",
      "Number of items: 333331\n"
     ]
    }
   ],
   "source": [
    "lines =[process(x) for x in lines]\n",
    "temp = []\n",
    "for line in lines:\n",
    "    temp+= [ x for x in line.split() ]\n",
    "lines = list(set(temp))\n",
    "print(\"\\n\".join(lines[:4]))\n",
    "print(\"Number of items:\",len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:10.311460Z",
     "start_time": "2023-12-01T11:15:10.288952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '0': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36}\n",
      "{0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: '0', 28: '1', 29: '2', 30: '3', 31: '4', 32: '5', 33: '6', 34: '7', 35: '8', 36: '9'}\n"
     ]
    }
   ],
   "source": [
    "# CHAR INDEXING\n",
    "char_set = list(\" abcdefghijklmnopqrstuvwxyz0123456789\")\n",
    "char2int = { char_set[x]:x for x in range(len(char_set)) }\n",
    "int2char = { char2int[x]:x for x in char_set }\n",
    "print(char2int)\n",
    "print(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:10.327981Z",
     "start_time": "2023-12-01T11:15:10.312630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '0': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36, '\\t': 37, '\\n': 38, '#': 39}\n",
      "{0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: '0', 28: '1', 29: '2', 30: '3', 31: '4', 32: '5', 33: '6', 34: '7', 35: '8', 36: '9', 37: '\\t', 38: '\\n', 39: '#'}\n"
     ]
    }
   ],
   "source": [
    "count = len(char_set)\n",
    "codes = [\"\\t\",\"\\n\",'#']\n",
    "for i in range(len(codes)):\n",
    "    code = codes[i]\n",
    "    char2int[code]=count\n",
    "    int2char[count]=code\n",
    "    count+=1\n",
    "print(char2int)\n",
    "print(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:10.391494Z",
     "start_time": "2023-12-01T11:15:10.336558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ficedula\n",
      "Gibberish: ficedyula\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#thresh - 0 to 1\n",
    "def gen_gibberish(line,thresh=0.2):\n",
    "    times = int(random.randrange(1,len(line)) * thresh)\n",
    "    '''\n",
    "    Types of replacement:\n",
    "        1.Delete random character.\n",
    "        2.Add random character.\n",
    "        3.Replace a character.\n",
    "        4.Combination?\n",
    "    '''\n",
    "    while times!=0:\n",
    "        # try to gen noise length times...\n",
    "        times-=1\n",
    "        val = random.randrange(0,10)\n",
    "        if val <= 5:\n",
    "            #get random index\n",
    "            val = random.randrange(0,10)\n",
    "            index = random.randrange(2,len(line))\n",
    "            if val <= 3 :\n",
    "                #delete character\n",
    "                line = line[:index]+line[index+1:]\n",
    "            else:\n",
    "                #add character\n",
    "                insert_index = random.randrange(0,len(char_set))\n",
    "                line = line[:index] + char_set[insert_index] + line[index:]\n",
    "        else:\n",
    "            index = random.randrange(0,len(char_set))\n",
    "            replace_index = random.randrange(2,len(line))\n",
    "            line = line[:replace_index] + char_set[index] + line[replace_index+1:]\n",
    "    return line\n",
    "\n",
    "sample = lines[5]\n",
    "gib = gen_gibberish(sample)\n",
    "print(\"Original:\",sample)\n",
    "print(\"Gibberish:\",gib)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:10.720625Z",
     "start_time": "2023-12-01T11:15:10.391494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN OF SAMPLES: 15303\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "REPEAT_FACTOR = 1\n",
    "SKIP = int(len(lines)*0.65)\n",
    "\n",
    "for line in lines[SKIP:]:\n",
    "    if len(line)>10:\n",
    "        output_text = '\\t' + line + '\\n'\n",
    "        for _ in range(REPEAT_FACTOR):\n",
    "            input_text = gen_gibberish(line)\n",
    "            input_texts.append(input_text)\n",
    "            target_texts.append(output_text)\n",
    "print(\"LEN OF SAMPLES:\",len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:10.753333Z",
     "start_time": "2023-12-01T11:15:10.729252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Enc Len: 39\n",
      "Max Dec Len: 38\n"
     ]
    }
   ],
   "source": [
    "max_enc_len = max([len(x) for x in input_texts])\n",
    "max_dec_len = max([len(x) for x in target_texts])\n",
    "print(\"Max Enc Len:\",max_enc_len)\n",
    "print(\"Max Dec Len:\",max_dec_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:10.788683Z",
     "start_time": "2023-12-01T11:15:10.756723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED ZERO VECTORS\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(input_texts)\n",
    "encoder_input_data = np.zeros( (num_samples , max_enc_len , len(char_set)),dtype='float32' )\n",
    "decoder_input_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\n",
    "decoder_target_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\n",
    "print(\"CREATED ZERO VECTORS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:11.528140Z",
     "start_time": "2023-12-01T11:15:10.796609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED...\n"
     ]
    }
   ],
   "source": [
    "#filling in the enc,dec datas\n",
    "for i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n",
    "    for t,char in enumerate(input_text):\n",
    "        encoder_input_data[ i , t , char2int[char] ] = 1\n",
    "    for t,char in enumerate(target_text):\n",
    "        decoder_input_data[ i, t , char2int[char] ] = 1\n",
    "        if t > 0 :\n",
    "            decoder_target_data[ i , t-1 , char2int[char] ] = 1\n",
    "print(\"COMPLETED...\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:24.305434Z",
     "start_time": "2023-12-01T11:15:11.531518Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input,LSTM,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:25.201406Z",
     "start_time": "2023-12-01T11:15:24.307946Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 250\n",
    "latent_dim = 256\n",
    "\n",
    "num_enc_tokens = len(char_set)\n",
    "num_dec_tokens = len(char_set) + 2 # includes \\n \\t\n",
    "encoder_inputs = Input(shape=(None,num_enc_tokens))\n",
    "encoder = LSTM(latent_dim,return_state=True)\n",
    "encoder_outputs , state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h,state_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T11:15:26.077166Z",
     "start_time": "2023-12-01T11:15:25.205657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 37)]   0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None, 39)]   0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 256),        301056      ['input_1[0][0]']                \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 256),  303104      ['input_2[0][0]',                \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 39)     10023       ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 614,183\n",
      "Trainable params: 614,183\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = Input(shape=(None,num_dec_tokens))\n",
    "decoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "decoder_ouputs,_,_ = decoder_lstm(decoder_inputs,initial_state = encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_dec_tokens, activation='softmax')\n",
    "decoder_ouputs = decoder_dense(decoder_ouputs)\n",
    "\n",
    "model = Model([encoder_inputs,decoder_inputs],decoder_ouputs)\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-01T11:15:07.711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "96/96 [==============================] - 63s 562ms/step - loss: 1.1013 - val_loss: 1.0544\n",
      "Epoch 2/250\n",
      "96/96 [==============================] - 51s 534ms/step - loss: 1.0554 - val_loss: 1.0359\n",
      "Epoch 3/250\n",
      "96/96 [==============================] - 51s 527ms/step - loss: 1.0489 - val_loss: 1.0399\n",
      "Epoch 4/250\n",
      "96/96 [==============================] - 51s 529ms/step - loss: 1.0429 - val_loss: 1.0287\n",
      "Epoch 5/250\n",
      "96/96 [==============================] - 52s 540ms/step - loss: 1.0368 - val_loss: 1.0356\n",
      "Epoch 6/250\n",
      "96/96 [==============================] - 51s 528ms/step - loss: 1.0306 - val_loss: 1.0202\n",
      "Epoch 7/250\n",
      "96/96 [==============================] - 51s 534ms/step - loss: 1.0234 - val_loss: 1.0121\n",
      "Epoch 8/250\n",
      "96/96 [==============================] - 52s 540ms/step - loss: 1.0158 - val_loss: 1.0090\n",
      "Epoch 9/250\n",
      "96/96 [==============================] - 52s 537ms/step - loss: 1.0102 - val_loss: 1.0000\n",
      "Epoch 10/250\n",
      "96/96 [==============================] - 51s 534ms/step - loss: 1.0046 - val_loss: 0.9983\n",
      "Epoch 11/250\n",
      "96/96 [==============================] - 51s 529ms/step - loss: 1.0002 - val_loss: 0.9924\n",
      "Epoch 12/250\n",
      "96/96 [==============================] - 50s 524ms/step - loss: 0.9953 - val_loss: 1.0010\n",
      "Epoch 13/250\n",
      "96/96 [==============================] - 50s 521ms/step - loss: 0.9919 - val_loss: 0.9922\n",
      "Epoch 14/250\n",
      "96/96 [==============================] - 54s 564ms/step - loss: 0.9885 - val_loss: 0.9888\n",
      "Epoch 15/250\n",
      "96/96 [==============================] - 56s 580ms/step - loss: 0.9845 - val_loss: 0.9758\n",
      "Epoch 16/250\n",
      "96/96 [==============================] - 54s 565ms/step - loss: 0.9813 - val_loss: 0.9755\n",
      "Epoch 17/250\n",
      "96/96 [==============================] - 51s 534ms/step - loss: 0.9790 - val_loss: 0.9796\n",
      "Epoch 18/250\n",
      "96/96 [==============================] - 51s 530ms/step - loss: 0.9762 - val_loss: 0.9782\n",
      "Epoch 19/250\n",
      "96/96 [==============================] - 52s 537ms/step - loss: 0.9743 - val_loss: 0.9622\n",
      "Epoch 20/250\n",
      "96/96 [==============================] - 50s 516ms/step - loss: 0.9722 - val_loss: 0.9685\n",
      "Epoch 21/250\n",
      "96/96 [==============================] - 50s 516ms/step - loss: 0.9701 - val_loss: 0.9643\n",
      "Epoch 22/250\n",
      "96/96 [==============================] - 50s 522ms/step - loss: 0.9687 - val_loss: 0.9719\n",
      "Epoch 23/250\n",
      "96/96 [==============================] - 52s 540ms/step - loss: 0.9662 - val_loss: 0.9697\n",
      "Epoch 24/250\n",
      "96/96 [==============================] - 52s 539ms/step - loss: 0.9654 - val_loss: 0.9577\n",
      "Epoch 25/250\n",
      "96/96 [==============================] - 52s 543ms/step - loss: 0.9637 - val_loss: 0.9570\n",
      "Epoch 26/250\n",
      "96/96 [==============================] - 53s 550ms/step - loss: 0.9622 - val_loss: 0.9557\n",
      "Epoch 27/250\n",
      "96/96 [==============================] - 52s 547ms/step - loss: 0.9607 - val_loss: 0.9531\n",
      "Epoch 28/250\n",
      "96/96 [==============================] - 53s 554ms/step - loss: 0.9593 - val_loss: 0.9665\n",
      "Epoch 29/250\n",
      "96/96 [==============================] - 54s 563ms/step - loss: 0.9588 - val_loss: 0.9551\n",
      "Epoch 30/250\n",
      "96/96 [==============================] - 53s 548ms/step - loss: 0.9569 - val_loss: 0.9539\n",
      "Epoch 31/250\n",
      "96/96 [==============================] - 51s 529ms/step - loss: 0.9559 - val_loss: 0.9507\n",
      "Epoch 32/250\n",
      "96/96 [==============================] - 50s 517ms/step - loss: 0.9547 - val_loss: 0.9535\n",
      "Epoch 33/250\n",
      "96/96 [==============================] - 51s 528ms/step - loss: 0.9532 - val_loss: 0.9513\n",
      "Epoch 34/250\n",
      "96/96 [==============================] - 50s 519ms/step - loss: 0.9525 - val_loss: 0.9535\n",
      "Epoch 35/250\n",
      "96/96 [==============================] - 54s 560ms/step - loss: 0.9511 - val_loss: 0.9515\n",
      "Epoch 36/250\n",
      "96/96 [==============================] - 52s 539ms/step - loss: 0.9498 - val_loss: 0.9429\n",
      "Epoch 37/250\n",
      "96/96 [==============================] - 50s 523ms/step - loss: 0.9481 - val_loss: 0.9450\n",
      "Epoch 38/250\n",
      "96/96 [==============================] - 50s 524ms/step - loss: 0.9453 - val_loss: 0.9320\n",
      "Epoch 39/250\n",
      "96/96 [==============================] - 53s 550ms/step - loss: 0.9408 - val_loss: 0.9386\n",
      "Epoch 40/250\n",
      "96/96 [==============================] - 51s 536ms/step - loss: 0.9453 - val_loss: 1.0399\n",
      "Epoch 41/250\n",
      "96/96 [==============================] - 53s 549ms/step - loss: 0.9679 - val_loss: 0.9409\n",
      "Epoch 42/250\n",
      "96/96 [==============================] - 51s 527ms/step - loss: 0.9446 - val_loss: 0.9375\n",
      "Epoch 43/250\n",
      "96/96 [==============================] - 50s 521ms/step - loss: 0.9432 - val_loss: 0.9367\n",
      "Epoch 44/250\n",
      "96/96 [==============================] - 50s 518ms/step - loss: 0.9414 - val_loss: 0.9325\n",
      "Epoch 45/250\n",
      "96/96 [==============================] - 50s 520ms/step - loss: 0.9372 - val_loss: 0.9294\n",
      "Epoch 46/250\n",
      "96/96 [==============================] - 50s 518ms/step - loss: 0.9410 - val_loss: 0.9354\n",
      "Epoch 47/250\n",
      "96/96 [==============================] - 51s 533ms/step - loss: 0.9865 - val_loss: 1.1704\n",
      "Epoch 48/250\n",
      "96/96 [==============================] - 51s 527ms/step - loss: 1.1229 - val_loss: 1.0222\n",
      "Epoch 49/250\n",
      "96/96 [==============================] - 52s 537ms/step - loss: 0.9682 - val_loss: 0.9474\n",
      "Epoch 50/250\n",
      "96/96 [==============================] - 55s 573ms/step - loss: 0.9462 - val_loss: 0.9367\n",
      "Epoch 51/250\n",
      "96/96 [==============================] - 50s 526ms/step - loss: 0.9424 - val_loss: 0.9337\n",
      "Epoch 52/250\n",
      "96/96 [==============================] - 50s 524ms/step - loss: 0.9399 - val_loss: 0.9346\n",
      "Epoch 53/250\n",
      "96/96 [==============================] - 49s 510ms/step - loss: 0.9551 - val_loss: 0.9467\n",
      "Epoch 54/250\n",
      "96/96 [==============================] - 50s 517ms/step - loss: 0.9389 - val_loss: 0.9316\n",
      "Epoch 55/250\n",
      "96/96 [==============================] - 50s 517ms/step - loss: 0.9362 - val_loss: 0.9353\n",
      "Epoch 56/250\n",
      "96/96 [==============================] - 52s 544ms/step - loss: 0.9335 - val_loss: 0.9317\n",
      "Epoch 57/250\n",
      "96/96 [==============================] - 50s 518ms/step - loss: 0.9293 - val_loss: 0.9278\n",
      "Epoch 58/250\n",
      "96/96 [==============================] - 51s 534ms/step - loss: 0.9261 - val_loss: 0.9220\n",
      "Epoch 59/250\n",
      "96/96 [==============================] - 50s 525ms/step - loss: 0.9282 - val_loss: 0.9273\n",
      "Epoch 60/250\n",
      "96/96 [==============================] - 50s 526ms/step - loss: 0.9247 - val_loss: 0.9206\n",
      "Epoch 61/250\n",
      "96/96 [==============================] - 53s 548ms/step - loss: 0.9193 - val_loss: 0.9115\n",
      "Epoch 62/250\n",
      "96/96 [==============================] - 52s 545ms/step - loss: 0.9542 - val_loss: 0.9526\n",
      "Epoch 63/250\n",
      "96/96 [==============================] - 51s 535ms/step - loss: 0.9407 - val_loss: 0.9341\n",
      "Epoch 64/250\n",
      "96/96 [==============================] - 52s 543ms/step - loss: 0.9281 - val_loss: 0.9320\n",
      "Epoch 65/250\n",
      "96/96 [==============================] - 52s 546ms/step - loss: 0.9232 - val_loss: 0.9307\n",
      "Epoch 66/250\n",
      "96/96 [==============================] - 54s 563ms/step - loss: 0.9166 - val_loss: 0.9165\n",
      "Epoch 67/250\n",
      "96/96 [==============================] - 50s 518ms/step - loss: 0.9110 - val_loss: 0.9077\n",
      "Epoch 68/250\n",
      "96/96 [==============================] - 51s 531ms/step - loss: 0.9060 - val_loss: 0.9126\n",
      "Epoch 69/250\n",
      "96/96 [==============================] - 50s 517ms/step - loss: 0.9035 - val_loss: 0.8990\n",
      "Epoch 70/250\n",
      "96/96 [==============================] - 53s 549ms/step - loss: 0.9016 - val_loss: 0.8993\n",
      "Epoch 71/250\n",
      "96/96 [==============================] - 54s 565ms/step - loss: 0.9048 - val_loss: 0.9060\n",
      "Epoch 72/250\n",
      "96/96 [==============================] - 55s 572ms/step - loss: 0.9015 - val_loss: 0.9092\n",
      "Epoch 73/250\n",
      "96/96 [==============================] - 51s 535ms/step - loss: 0.8987 - val_loss: 0.9044\n",
      "Epoch 74/250\n",
      "96/96 [==============================] - 52s 545ms/step - loss: 0.8969 - val_loss: 0.8936\n",
      "Epoch 75/250\n",
      "96/96 [==============================] - 51s 529ms/step - loss: 0.8913 - val_loss: 0.8925\n",
      "Epoch 76/250\n",
      "96/96 [==============================] - 52s 537ms/step - loss: 0.8872 - val_loss: 0.8904\n",
      "Epoch 77/250\n",
      "96/96 [==============================] - 51s 534ms/step - loss: 0.8826 - val_loss: 0.8869\n",
      "Epoch 78/250\n",
      "96/96 [==============================] - 51s 534ms/step - loss: 0.8807 - val_loss: 0.8843\n",
      "Epoch 79/250\n",
      "96/96 [==============================] - 50s 525ms/step - loss: 0.8761 - val_loss: 0.8774\n",
      "Epoch 80/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 51s 527ms/step - loss: 0.8743 - val_loss: 0.8718\n",
      "Epoch 81/250\n",
      "96/96 [==============================] - 53s 552ms/step - loss: 0.8690 - val_loss: 0.8774\n",
      "Epoch 82/250\n",
      "96/96 [==============================] - 53s 548ms/step - loss: 0.8659 - val_loss: 0.8653\n",
      "Epoch 83/250\n",
      "96/96 [==============================] - 51s 535ms/step - loss: 0.8649 - val_loss: 0.8688\n",
      "Epoch 84/250\n",
      "96/96 [==============================] - 50s 526ms/step - loss: 0.8593 - val_loss: 0.8634\n",
      "Epoch 85/250\n",
      "96/96 [==============================] - 51s 531ms/step - loss: 0.8568 - val_loss: 0.8638\n",
      "Epoch 86/250\n",
      "96/96 [==============================] - 49s 514ms/step - loss: 0.8548 - val_loss: 0.8556\n",
      "Epoch 87/250\n",
      "96/96 [==============================] - 50s 524ms/step - loss: 0.8553 - val_loss: 0.8554\n",
      "Epoch 88/250\n",
      "96/96 [==============================] - 49s 514ms/step - loss: 0.8518 - val_loss: 0.8571\n",
      "Epoch 89/250\n",
      "96/96 [==============================] - 49s 516ms/step - loss: 0.8511 - val_loss: 0.8548\n",
      "Epoch 90/250\n",
      "96/96 [==============================] - 52s 542ms/step - loss: 0.8518 - val_loss: 0.8508\n",
      "Epoch 91/250\n",
      "96/96 [==============================] - 49s 514ms/step - loss: 0.8464 - val_loss: 0.8493\n",
      "Epoch 92/250\n",
      "96/96 [==============================] - 52s 541ms/step - loss: 0.8439 - val_loss: 0.8469\n",
      "Epoch 93/250\n",
      "96/96 [==============================] - 53s 547ms/step - loss: 0.8440 - val_loss: 0.8593\n",
      "Epoch 94/250\n",
      "96/96 [==============================] - 50s 522ms/step - loss: 0.8451 - val_loss: 0.8468\n",
      "Epoch 95/250\n",
      "96/96 [==============================] - 50s 519ms/step - loss: 0.8387 - val_loss: 0.8489\n",
      "Epoch 96/250\n",
      "96/96 [==============================] - 52s 544ms/step - loss: 0.8372 - val_loss: 0.8448\n",
      "Epoch 97/250\n",
      "96/96 [==============================] - 51s 528ms/step - loss: 0.8340 - val_loss: 0.8465\n",
      "Epoch 98/250\n",
      "96/96 [==============================] - 52s 541ms/step - loss: 0.8358 - val_loss: 0.8502\n",
      "Epoch 99/250\n",
      "96/96 [==============================] - 51s 527ms/step - loss: 0.8328 - val_loss: 0.8376\n",
      "Epoch 100/250\n",
      "96/96 [==============================] - 52s 537ms/step - loss: 0.8303 - val_loss: 0.8382\n",
      "Epoch 101/250\n",
      "96/96 [==============================] - 51s 531ms/step - loss: 0.8307 - val_loss: 0.8438\n",
      "Epoch 102/250\n",
      "96/96 [==============================] - 51s 535ms/step - loss: 0.8363 - val_loss: 0.8531\n",
      "Epoch 103/250\n",
      "96/96 [==============================] - 50s 522ms/step - loss: 0.8349 - val_loss: 0.8367\n",
      "Epoch 104/250\n",
      "96/96 [==============================] - 50s 525ms/step - loss: 0.8294 - val_loss: 0.8350\n",
      "Epoch 105/250\n",
      "96/96 [==============================] - 52s 541ms/step - loss: 0.8281 - val_loss: 0.8499\n",
      "Epoch 106/250\n",
      "96/96 [==============================] - 51s 530ms/step - loss: 0.8258 - val_loss: 0.8386\n",
      "Epoch 107/250\n",
      "96/96 [==============================] - 51s 533ms/step - loss: 0.8235 - val_loss: 0.8436\n",
      "Epoch 108/250\n",
      "96/96 [==============================] - 51s 531ms/step - loss: 0.8264 - val_loss: 0.8439\n",
      "Epoch 109/250\n",
      "96/96 [==============================] - 52s 537ms/step - loss: 0.8236 - val_loss: 0.8281\n",
      "Epoch 110/250\n",
      "96/96 [==============================] - 50s 518ms/step - loss: 0.8221 - val_loss: 0.8243\n",
      "Epoch 111/250\n",
      "96/96 [==============================] - 51s 530ms/step - loss: 0.8187 - val_loss: 0.8290\n",
      "Epoch 112/250\n",
      "96/96 [==============================] - 51s 529ms/step - loss: 0.8162 - val_loss: 0.8210\n",
      "Epoch 113/250\n",
      "96/96 [==============================] - 52s 537ms/step - loss: 0.8142 - val_loss: 0.8257\n",
      "Epoch 114/250\n",
      "96/96 [==============================] - 53s 553ms/step - loss: 0.8137 - val_loss: 0.8301\n",
      "Epoch 115/250\n",
      "96/96 [==============================] - 51s 532ms/step - loss: 0.8155 - val_loss: 0.8145\n",
      "Epoch 116/250\n",
      "96/96 [==============================] - 52s 538ms/step - loss: 0.8081 - val_loss: 0.8166\n",
      "Epoch 117/250\n",
      "96/96 [==============================] - 50s 519ms/step - loss: 0.8051 - val_loss: 0.8146\n",
      "Epoch 118/250\n",
      "96/96 [==============================] - 50s 522ms/step - loss: 0.8037 - val_loss: 0.8164\n",
      "Epoch 119/250\n",
      "96/96 [==============================] - 51s 533ms/step - loss: 0.8029 - val_loss: 0.8188\n",
      "Epoch 120/250\n",
      "96/96 [==============================] - 53s 550ms/step - loss: 0.8006 - val_loss: 0.8159\n",
      "Epoch 121/250\n",
      "96/96 [==============================] - 51s 535ms/step - loss: 0.8000 - val_loss: 0.8200\n",
      "Epoch 122/250\n",
      "96/96 [==============================] - 52s 538ms/step - loss: 0.7971 - val_loss: 0.8189\n",
      "Epoch 123/250\n",
      "96/96 [==============================] - 52s 543ms/step - loss: 0.7940 - val_loss: 0.8014\n",
      "Epoch 124/250\n",
      "96/96 [==============================] - 50s 526ms/step - loss: 0.7918 - val_loss: 0.8090\n",
      "Epoch 125/250\n",
      "96/96 [==============================] - 51s 530ms/step - loss: 0.7912 - val_loss: 0.8103\n",
      "Epoch 126/250\n",
      "96/96 [==============================] - 51s 537ms/step - loss: 0.7887 - val_loss: 0.7975\n",
      "Epoch 127/250\n",
      "96/96 [==============================] - 54s 558ms/step - loss: 0.7841 - val_loss: 0.7985\n",
      "Epoch 128/250\n",
      "96/96 [==============================] - 51s 532ms/step - loss: 0.7834 - val_loss: 0.7914\n",
      "Epoch 129/250\n",
      "96/96 [==============================] - 52s 538ms/step - loss: 0.7796 - val_loss: 0.7876\n",
      "Epoch 130/250\n",
      "96/96 [==============================] - 50s 522ms/step - loss: 0.7810 - val_loss: 0.7929\n",
      "Epoch 131/250\n",
      "96/96 [==============================] - 42s 437ms/step - loss: 0.7803 - val_loss: 0.7974\n",
      "Epoch 132/250\n",
      "96/96 [==============================] - 40s 417ms/step - loss: 0.7754 - val_loss: 0.7849\n",
      "Epoch 133/250\n",
      "96/96 [==============================] - 38s 394ms/step - loss: 0.7711 - val_loss: 0.7838\n",
      "Epoch 134/250\n",
      "96/96 [==============================] - 34s 355ms/step - loss: 0.7685 - val_loss: 0.7817\n",
      "Epoch 135/250\n",
      "96/96 [==============================] - 34s 354ms/step - loss: 0.7664 - val_loss: 0.7759\n",
      "Epoch 136/250\n",
      "96/96 [==============================] - 34s 355ms/step - loss: 0.7632 - val_loss: 0.7771\n",
      "Epoch 137/250\n",
      "96/96 [==============================] - 38s 402ms/step - loss: 0.7609 - val_loss: 0.7759\n",
      "Epoch 138/250\n",
      "96/96 [==============================] - 38s 399ms/step - loss: 0.7595 - val_loss: 0.7709\n",
      "Epoch 139/250\n",
      "96/96 [==============================] - 37s 384ms/step - loss: 0.7568 - val_loss: 0.7683\n",
      "Epoch 140/250\n",
      "96/96 [==============================] - 36s 374ms/step - loss: 0.7541 - val_loss: 0.7685\n",
      "Epoch 141/250\n",
      "96/96 [==============================] - 35s 365ms/step - loss: 0.7505 - val_loss: 0.7695\n",
      "Epoch 142/250\n",
      "96/96 [==============================] - 36s 373ms/step - loss: 0.7484 - val_loss: 0.7648\n",
      "Epoch 143/250\n",
      "96/96 [==============================] - 35s 363ms/step - loss: 0.7453 - val_loss: 0.7575\n",
      "Epoch 144/250\n",
      "96/96 [==============================] - 35s 363ms/step - loss: 0.7424 - val_loss: 0.7577\n",
      "Epoch 145/250\n",
      "96/96 [==============================] - 37s 382ms/step - loss: 0.7390 - val_loss: 0.7537\n",
      "Epoch 146/250\n",
      "96/96 [==============================] - 37s 382ms/step - loss: 0.7364 - val_loss: 0.7474\n",
      "Epoch 147/250\n",
      "96/96 [==============================] - 36s 376ms/step - loss: 0.7337 - val_loss: 0.7482\n",
      "Epoch 148/250\n",
      "96/96 [==============================] - 35s 369ms/step - loss: 0.7305 - val_loss: 0.7460\n",
      "Epoch 149/250\n",
      "96/96 [==============================] - 36s 374ms/step - loss: 0.7280 - val_loss: 0.7475\n",
      "Epoch 150/250\n",
      "96/96 [==============================] - 36s 371ms/step - loss: 0.7257 - val_loss: 0.7378\n",
      "Epoch 151/250\n",
      "96/96 [==============================] - 36s 370ms/step - loss: 0.7245 - val_loss: 0.7378\n",
      "Epoch 152/250\n",
      "96/96 [==============================] - 36s 373ms/step - loss: 0.7215 - val_loss: 0.7418\n",
      "Epoch 153/250\n",
      "96/96 [==============================] - 36s 372ms/step - loss: 0.7181 - val_loss: 0.7377\n",
      "Epoch 154/250\n",
      "96/96 [==============================] - 36s 373ms/step - loss: 0.7167 - val_loss: 0.7326\n",
      "Epoch 155/250\n",
      "96/96 [==============================] - 36s 376ms/step - loss: 0.7122 - val_loss: 0.7388\n",
      "Epoch 156/250\n",
      "96/96 [==============================] - 36s 372ms/step - loss: 0.7150 - val_loss: 0.7336\n",
      "Epoch 157/250\n",
      "96/96 [==============================] - 36s 373ms/step - loss: 0.7139 - val_loss: 0.7267\n",
      "Epoch 158/250\n",
      "96/96 [==============================] - 36s 373ms/step - loss: 0.7085 - val_loss: 0.7278\n",
      "Epoch 159/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 35s 362ms/step - loss: 0.7110 - val_loss: 0.7238\n",
      "Epoch 160/250\n",
      "96/96 [==============================] - 35s 366ms/step - loss: 0.7059 - val_loss: 0.7239\n",
      "Epoch 161/250\n",
      "96/96 [==============================] - 36s 375ms/step - loss: 0.7037 - val_loss: 0.7222\n",
      "Epoch 162/250\n",
      "96/96 [==============================] - 35s 367ms/step - loss: 0.7064 - val_loss: 0.7206\n",
      "Epoch 163/250\n",
      "96/96 [==============================] - 36s 370ms/step - loss: 0.7001 - val_loss: 0.7203\n",
      "Epoch 164/250\n",
      "96/96 [==============================] - 36s 372ms/step - loss: 0.6991 - val_loss: 0.7179\n",
      "Epoch 165/250\n",
      "96/96 [==============================] - 35s 370ms/step - loss: 0.6950 - val_loss: 0.7143\n",
      "Epoch 166/250\n",
      "96/96 [==============================] - 35s 368ms/step - loss: 0.6890 - val_loss: 0.7095\n",
      "Epoch 167/250\n",
      "96/96 [==============================] - 35s 369ms/step - loss: 0.6897 - val_loss: 0.7076\n",
      "Epoch 168/250\n",
      "96/96 [==============================] - 36s 372ms/step - loss: 0.6904 - val_loss: 0.7044\n",
      "Epoch 169/250\n",
      "96/96 [==============================] - 35s 369ms/step - loss: 0.6845 - val_loss: 0.7106\n",
      "Epoch 170/250\n",
      "96/96 [==============================] - 37s 386ms/step - loss: 0.6814 - val_loss: 0.7022\n",
      "Epoch 171/250\n",
      "96/96 [==============================] - 36s 378ms/step - loss: 0.6798 - val_loss: 0.7023\n",
      "Epoch 172/250\n",
      "96/96 [==============================] - 36s 374ms/step - loss: 0.6774 - val_loss: 0.6909\n",
      "Epoch 173/250\n",
      "96/96 [==============================] - 36s 370ms/step - loss: 0.6733 - val_loss: 0.6894\n",
      "Epoch 174/250\n",
      "96/96 [==============================] - 36s 372ms/step - loss: 0.6723 - val_loss: 0.6983\n",
      "Epoch 175/250\n",
      "96/96 [==============================] - 35s 370ms/step - loss: 0.6701 - val_loss: 0.6893\n",
      "Epoch 176/250\n",
      "96/96 [==============================] - 35s 370ms/step - loss: 0.6682 - val_loss: 0.6911\n",
      "Epoch 177/250\n",
      "96/96 [==============================] - 35s 368ms/step - loss: 0.6643 - val_loss: 0.6857\n",
      "Epoch 178/250\n",
      "96/96 [==============================] - 35s 367ms/step - loss: 0.6616 - val_loss: 0.6899\n",
      "Epoch 179/250\n",
      "96/96 [==============================] - 35s 369ms/step - loss: 0.6588 - val_loss: 0.6809\n",
      "Epoch 180/250\n",
      "96/96 [==============================] - 35s 364ms/step - loss: 0.6559 - val_loss: 0.6783\n",
      "Epoch 181/250\n",
      "96/96 [==============================] - 36s 375ms/step - loss: 0.6543 - val_loss: 0.6788\n",
      "Epoch 182/250\n",
      "96/96 [==============================] - 36s 372ms/step - loss: 0.6520 - val_loss: 0.6696\n",
      "Epoch 183/250\n",
      "96/96 [==============================] - 36s 372ms/step - loss: 0.6488 - val_loss: 0.6877\n",
      "Epoch 184/250\n",
      "96/96 [==============================] - 36s 371ms/step - loss: 0.6471 - val_loss: 0.6628\n",
      "Epoch 185/250\n",
      "96/96 [==============================] - 36s 371ms/step - loss: 0.6454 - val_loss: 0.6670\n",
      "Epoch 186/250\n",
      "96/96 [==============================] - 36s 374ms/step - loss: 0.6417 - val_loss: 0.6651\n",
      "Epoch 187/250\n",
      "96/96 [==============================] - 36s 371ms/step - loss: 0.6390 - val_loss: 0.6659\n",
      "Epoch 188/250\n",
      "96/96 [==============================] - 36s 376ms/step - loss: 0.6377 - val_loss: 0.6636\n",
      "Epoch 189/250\n",
      "96/96 [==============================] - 37s 381ms/step - loss: 0.6369 - val_loss: 0.6644\n",
      "Epoch 190/250\n",
      "96/96 [==============================] - 38s 392ms/step - loss: 0.6327 - val_loss: 0.6573\n",
      "Epoch 191/250\n",
      "96/96 [==============================] - 38s 393ms/step - loss: 0.6293 - val_loss: 0.6514\n",
      "Epoch 192/250\n",
      "96/96 [==============================] - 38s 396ms/step - loss: 0.6262 - val_loss: 0.6519\n",
      "Epoch 193/250\n",
      "96/96 [==============================] - 37s 389ms/step - loss: 0.6240 - val_loss: 0.6759\n",
      "Epoch 194/250\n",
      "96/96 [==============================] - 38s 396ms/step - loss: 0.6217 - val_loss: 0.6455\n",
      "Epoch 195/250\n",
      "96/96 [==============================] - 37s 389ms/step - loss: 0.6215 - val_loss: 0.6559\n",
      "Epoch 196/250\n",
      "96/96 [==============================] - 38s 392ms/step - loss: 0.6193 - val_loss: 0.6476\n",
      "Epoch 197/250\n",
      "96/96 [==============================] - 38s 396ms/step - loss: 0.6156 - val_loss: 0.6565\n",
      "Epoch 198/250\n",
      "96/96 [==============================] - 37s 389ms/step - loss: 0.6146 - val_loss: 0.6412\n",
      "Epoch 199/250\n",
      "96/96 [==============================] - 37s 387ms/step - loss: 0.6123 - val_loss: 0.6496\n",
      "Epoch 200/250\n",
      "96/96 [==============================] - 39s 407ms/step - loss: 0.6083 - val_loss: 0.6405\n",
      "Epoch 201/250\n",
      "96/96 [==============================] - 36s 375ms/step - loss: 0.6057 - val_loss: 0.6420\n",
      "Epoch 202/250\n",
      "96/96 [==============================] - 36s 378ms/step - loss: 0.6065 - val_loss: 0.6330\n",
      "Epoch 203/250\n",
      "96/96 [==============================] - 36s 376ms/step - loss: 0.6045 - val_loss: 0.6268\n",
      "Epoch 204/250\n",
      "96/96 [==============================] - 35s 370ms/step - loss: 0.5998 - val_loss: 0.6249\n",
      "Epoch 205/250\n",
      "96/96 [==============================] - 34s 357ms/step - loss: 0.5971 - val_loss: 0.6261\n",
      "Epoch 206/250\n",
      "96/96 [==============================] - 34s 358ms/step - loss: 0.5947 - val_loss: 0.6198\n",
      "Epoch 207/250\n",
      "96/96 [==============================] - 35s 362ms/step - loss: 0.5954 - val_loss: 0.6235\n",
      "Epoch 208/250\n",
      "96/96 [==============================] - 34s 358ms/step - loss: 0.5912 - val_loss: 0.6197\n",
      "Epoch 209/250\n",
      "96/96 [==============================] - 35s 360ms/step - loss: 0.5889 - val_loss: 0.6218\n",
      "Epoch 210/250\n",
      "96/96 [==============================] - 35s 360ms/step - loss: 0.5876 - val_loss: 0.6172\n",
      "Epoch 211/250\n",
      "96/96 [==============================] - 34s 360ms/step - loss: 0.5869 - val_loss: 0.6167\n",
      "Epoch 212/250\n",
      "96/96 [==============================] - 34s 358ms/step - loss: 0.5853 - val_loss: 0.6262\n",
      "Epoch 213/250\n",
      "96/96 [==============================] - 34s 359ms/step - loss: 0.5812 - val_loss: 0.6159\n",
      "Epoch 214/250\n",
      "96/96 [==============================] - 35s 363ms/step - loss: 0.5798 - val_loss: 0.6104\n",
      "Epoch 215/250\n",
      "96/96 [==============================] - 38s 392ms/step - loss: 0.5773 - val_loss: 0.6127\n",
      "Epoch 216/250\n",
      "96/96 [==============================] - 37s 389ms/step - loss: 0.5752 - val_loss: 0.6078\n",
      "Epoch 217/250\n",
      "96/96 [==============================] - 36s 380ms/step - loss: 0.5732 - val_loss: 0.6064\n",
      "Epoch 218/250\n",
      "96/96 [==============================] - 36s 379ms/step - loss: 0.5720 - val_loss: 0.6116\n",
      "Epoch 219/250\n",
      "96/96 [==============================] - 36s 379ms/step - loss: 0.5720 - val_loss: 0.6054\n",
      "Epoch 220/250\n",
      "96/96 [==============================] - 38s 400ms/step - loss: 0.5758 - val_loss: 0.6257\n",
      "Epoch 221/250\n",
      "96/96 [==============================] - 38s 391ms/step - loss: 0.5727 - val_loss: 0.6035\n",
      "Epoch 222/250\n",
      "96/96 [==============================] - 36s 378ms/step - loss: 0.5665 - val_loss: 0.6008\n",
      "Epoch 223/250\n",
      "96/96 [==============================] - 36s 374ms/step - loss: 0.5645 - val_loss: 0.5997\n",
      "Epoch 224/250\n",
      "96/96 [==============================] - 37s 383ms/step - loss: 0.5642 - val_loss: 0.5988\n",
      "Epoch 225/250\n",
      "96/96 [==============================] - 37s 381ms/step - loss: 0.5606 - val_loss: 0.6062\n",
      "Epoch 226/250\n",
      "96/96 [==============================] - 36s 379ms/step - loss: 0.5589 - val_loss: 0.5956\n",
      "Epoch 227/250\n",
      "96/96 [==============================] - 36s 376ms/step - loss: 0.5575 - val_loss: 0.6033\n",
      "Epoch 228/250\n",
      "20/96 [=====>........................] - ETA: 25s - loss: 0.5546"
     ]
    }
   ],
   "source": [
    "h=model.fit([encoder_input_data,decoder_input_data],decoder_target_data\n",
    "         ,epochs = epochs,\n",
    "          batch_size = batch_size,\n",
    "          validation_split = 0.2\n",
    "         )\n",
    "model.save('s2s.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-01T11:15:07.714Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(h.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-01T13:05:52.081Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs,encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\n",
    "decoder_outputs,state_h,state_c = decoder_lstm(\n",
    "        decoder_inputs,initial_state = decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h,state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n",
    "encoder_model.save('encoder.h5')\n",
    "decoder_model.save('decoder.h5')\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_dec_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, char2int['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = int2char[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_dec_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_dec_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "for seq_index in range(700):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Wrong sentence:', input_texts[seq_index])\n",
    "    print('Corrected sentence:', decoded_sentence)\n",
    "    print('Ground Truth:',target_texts[seq_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2367,
     "sourceId": 3976,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 28772,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
