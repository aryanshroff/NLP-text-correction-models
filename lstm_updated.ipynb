{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3976,"sourceType":"datasetVersion","datasetId":2367}],"dockerImageVersionId":28772,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\nimport Levenshtein\nimport nltk\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-05T13:53:38.926118Z","iopub.execute_input":"2023-12-05T13:53:38.926448Z","iopub.status.idle":"2023-12-05T13:53:40.638437Z","shell.execute_reply.started":"2023-12-05T13:53:38.926392Z","shell.execute_reply":"2023-12-05T13:53:40.637680Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"['unigram_freq.csv']\n","output_type":"stream"}]},{"cell_type":"code","source":"# File loading\ndf  = pd.read_csv('../input/unigram_freq.csv')\nprint(df.shape)\ndf.dropna(axis=0,how='any')\nprint(df.shape)\n","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2023-12-05T13:53:40.640208Z","iopub.execute_input":"2023-12-05T13:53:40.640448Z","iopub.status.idle":"2023-12-05T13:53:41.010439Z","shell.execute_reply.started":"2023-12-05T13:53:40.640410Z","shell.execute_reply":"2023-12-05T13:53:41.009501Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(333333, 2)\n(333333, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"lines = [x for x in df['word'] if type(x) == type('a') ]\nprint(\"Line Count:\",len(lines))\nprint(lines[:4])","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:41.011926Z","iopub.execute_input":"2023-12-05T13:53:41.012263Z","iopub.status.idle":"2023-12-05T13:53:41.122456Z","shell.execute_reply.started":"2023-12-05T13:53:41.012205Z","shell.execute_reply":"2023-12-05T13:53:41.121570Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Line Count: 333331\n['the', 'of', 'and', 'to']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Preprocessing\nimport re\ndef process(sent):\n    sent=sent.lower()\n    sent=re.sub(r'[^0-9a-zA-Z ]','',sent)\n    sent=sent.replace('\\n','')\n    return sent    ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:41.124043Z","iopub.execute_input":"2023-12-05T13:53:41.124382Z","iopub.status.idle":"2023-12-05T13:53:41.129261Z","shell.execute_reply.started":"2023-12-05T13:53:41.124325Z","shell.execute_reply":"2023-12-05T13:53:41.128509Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"lines =[process(x) for x in lines]\ntemp = []\nfor line in lines:\n    temp+= [ x for x in line.split() ]\nlines = list(set(temp))\nprint(\"\\n\".join(lines[:4]))\nprint(\"Number of items:\",len(lines))","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:41.132921Z","iopub.execute_input":"2023-12-05T13:53:41.133251Z","iopub.status.idle":"2023-12-05T13:53:42.201640Z","shell.execute_reply.started":"2023-12-05T13:53:41.133191Z","shell.execute_reply":"2023-12-05T13:53:42.200799Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"mayflower\nangele\nspinola\nuctuations\nNumber of items: 333331\n","output_type":"stream"}]},{"cell_type":"code","source":"# CHAR INDEXING\nchar_set = list(\" abcdefghijklmnopqrstuvwxyz0123456789',./?\")\nchar2int = { char_set[x]:x for x in range(len(char_set)) }\nint2char = { char2int[x]:x for x in char_set }\nprint(char2int)\nprint(int2char)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:42.204531Z","iopub.execute_input":"2023-12-05T13:53:42.204866Z","iopub.status.idle":"2023-12-05T13:53:42.211344Z","shell.execute_reply.started":"2023-12-05T13:53:42.204809Z","shell.execute_reply":"2023-12-05T13:53:42.210418Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '0': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36, \"'\": 37, ',': 38, '.': 39, '/': 40, '?': 41}\n{0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: '0', 28: '1', 29: '2', 30: '3', 31: '4', 32: '5', 33: '6', 34: '7', 35: '8', 36: '9', 37: \"'\", 38: ',', 39: '.', 40: '/', 41: '?'}\n","output_type":"stream"}]},{"cell_type":"code","source":"count = len(char_set)\ncodes = [\"\\t\",\"\\n\",'#']\nfor i in range(len(codes)):\n    code = codes[i]\n    char2int[code]=count\n    int2char[count]=code\n    count+=1\nprint(char2int)\nprint(int2char)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:42.212762Z","iopub.execute_input":"2023-12-05T13:53:42.213078Z","iopub.status.idle":"2023-12-05T13:53:42.222367Z","shell.execute_reply.started":"2023-12-05T13:53:42.213024Z","shell.execute_reply":"2023-12-05T13:53:42.221594Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '0': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36, \"'\": 37, ',': 38, '.': 39, '/': 40, '?': 41, '\\t': 42, '\\n': 43, '#': 44}\n{0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: '0', 28: '1', 29: '2', 30: '3', 31: '4', 32: '5', 33: '6', 34: '7', 35: '8', 36: '9', 37: \"'\", 38: ',', 39: '.', 40: '/', 41: '?', 42: '\\t', 43: '\\n', 44: '#'}\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\n#thresh - 0 to 1\ndef gen_gibberish(line,thresh=0.2):\n    times = int(random.randrange(1,len(line)) * thresh)\n    '''\n    Types of replacement:\n        1.Delete random character.\n        2.Add random character.\n        3.Replace a character.\n        4.Combination?\n    '''\n    while times!=0:\n        # try to gen noise length times...\n        times-=1\n        val = random.randrange(0,10)\n        if val <= 5:\n            #get random index\n            val = random.randrange(0,10)\n            index = random.randrange(2,len(line))\n            if val <= 3 :\n                #delete character\n                line = line[:index]+line[index+1:]\n            else:\n                #add character\n                insert_index = random.randrange(0,len(char_set))\n                line = line[:index] + char_set[insert_index] + line[index:]\n        else:\n            index = random.randrange(0,len(char_set))\n            replace_index = random.randrange(2,len(line))\n            line = line[:replace_index] + char_set[index] + line[replace_index+1:]\n    return line\n\nsample = lines[5]\ngib = gen_gibberish(sample)\nprint(\"Original:\",sample)\nprint(\"Gibberish:\",gib)\nprint(len(lines))        \n#print(lines)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:42.223788Z","iopub.execute_input":"2023-12-05T13:53:42.224108Z","iopub.status.idle":"2023-12-05T13:53:42.248038Z","shell.execute_reply.started":"2023-12-05T13:53:42.224057Z","shell.execute_reply":"2023-12-05T13:53:42.247253Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Original: theresa\nGibberish: thgresa\n333331\n","output_type":"stream"}]},{"cell_type":"code","source":"def alter():\n    corrected_text=\"This is a wrong sentence with errors in spelling and grammatical mistakes. It has incorrect punctuations, missing letters, and wrong word orders. The grammar and spelling are all over the place. The sentences don't make sense, and the words are jumbled up. There are run-on sentences, and many words are misspelled. It's a complete chaos and hard to understand. The wrong sentence is full of typos, and it makes it difficult to read. There's no proper grammar, and the punctuation is all wrong too. It is a mess of words and it's difficult to understand the meaning. The wrong sentence is a great example of how not to write. It's full of errors, and it makes it hard to read. The wrong sentence is a disaster in the land of language. It is a hot mess of words, and it's hard to make any sense of it. The grammar is all over the place, and the spelling is wrong. It is a disaster of a sentence and it is difficult to comprehend. The wrong sentence is a complete garble of letters. It is hard to read, and it is full of mistakes. The word order is all out of place, and the punctuation is incorrect. There's no coherence or structure in the wrong sentence. It's a jumbled mess of incoherent words and it is confusing to read. The wrong sentence is a nightmare for any reader. It is a cacophony of errors, and it is hard to decipher. The wrong sentence is a total disaster, and it is impossible to understand. The grammar is terrible, and the spelling is all over the place. It's a jumble of letters, and it is incomprehensible. The wrong sentence is a train wreck of words, and it is a pain to read. It is full of errors, and it is a complete mess. The wrong sentence is a chaos of typos, and it is difficult to follow.\"\n    corrected_text= corrected_text.lower()\n    #print(corrected_text)\n    true_labels = nltk.word_tokenize(corrected_text)\n    print(true_labels)\n    lines=true_labels\n    return lines\n#lines=alter()    ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:42.249702Z","iopub.execute_input":"2023-12-05T13:53:42.249987Z","iopub.status.idle":"2023-12-05T13:53:42.260414Z","shell.execute_reply.started":"2023-12-05T13:53:42.249938Z","shell.execute_reply":"2023-12-05T13:53:42.259456Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#print(len(lines))\n# create dataset\ninput_texts = []\ntarget_texts = []\nREPEAT_FACTOR = 1\nSKIP = int(len(lines)*0.65)\n\nfor line in lines[SKIP:]:\n    if len(line)>1:\n        output_text = '\\t' + line + '\\n'\n        for _ in range(REPEAT_FACTOR):\n            input_text = gen_gibberish(line)\n            input_texts.append(input_text)\n            target_texts.append(output_text)\nprint(\"LEN OF SAMPLES:\",len(input_texts))\n\n#print(input_texts)\n#print(target_texts)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:42.261746Z","iopub.execute_input":"2023-12-05T13:53:42.262016Z","iopub.status.idle":"2023-12-05T13:53:43.104909Z","shell.execute_reply.started":"2023-12-05T13:53:42.261968Z","shell.execute_reply":"2023-12-05T13:53:43.104110Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"LEN OF SAMPLES: 116662\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_enc_len = max([len(x) for x in input_texts])\nmax_dec_len = max([len(x) for x in target_texts])\nprint(\"Max Enc Len:\",max_enc_len)\nprint(\"Max Dec Len:\",max_dec_len)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:43.106102Z","iopub.execute_input":"2023-12-05T13:53:43.106377Z","iopub.status.idle":"2023-12-05T13:53:43.150282Z","shell.execute_reply.started":"2023-12-05T13:53:43.106329Z","shell.execute_reply":"2023-12-05T13:53:43.149502Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Max Enc Len: 36\nMax Dec Len: 40\n","output_type":"stream"}]},{"cell_type":"code","source":"num_samples = len(input_texts)\nencoder_input_data = np.zeros( (num_samples , max_enc_len , len(char_set)),dtype='float32' )\ndecoder_input_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\ndecoder_target_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\nprint(\"CREATED ZERO VECTORS\")","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:43.151497Z","iopub.execute_input":"2023-12-05T13:53:43.151762Z","iopub.status.idle":"2023-12-05T13:53:43.158520Z","shell.execute_reply.started":"2023-12-05T13:53:43.151717Z","shell.execute_reply":"2023-12-05T13:53:43.157804Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"CREATED ZERO VECTORS\n","output_type":"stream"}]},{"cell_type":"code","source":"print(char2int)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:43.160186Z","iopub.execute_input":"2023-12-05T13:53:43.160643Z","iopub.status.idle":"2023-12-05T13:53:43.167711Z","shell.execute_reply.started":"2023-12-05T13:53:43.160440Z","shell.execute_reply":"2023-12-05T13:53:43.166856Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '0': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36, \"'\": 37, ',': 38, '.': 39, '/': 40, '?': 41, '\\t': 42, '\\n': 43, '#': 44}\n","output_type":"stream"}]},{"cell_type":"code","source":"#filling in the enc,dec datas\nfor i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n    #input_text=input_text.lower()\n    for t,char in enumerate(input_text):\n        #print(input_text)\n        #char = char.lower()\n        encoder_input_data[ i , t , char2int[char] ] = 1\n    for t,char in enumerate(target_text):\n        decoder_input_data[ i, t , char2int[char] ] = 1\n        if t > 0 :\n            decoder_target_data[ i , t-1 , char2int[char] ] = 1\nprint(\"COMPLETED...\")    ","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:43.168725Z","iopub.execute_input":"2023-12-05T13:53:43.168968Z","iopub.status.idle":"2023-12-05T13:53:46.038855Z","shell.execute_reply.started":"2023-12-05T13:53:43.168927Z","shell.execute_reply":"2023-12-05T13:53:46.038010Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"COMPLETED...\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input,LSTM,Dense","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:46.040313Z","iopub.execute_input":"2023-12-05T13:53:46.040647Z","iopub.status.idle":"2023-12-05T13:53:47.883193Z","shell.execute_reply.started":"2023-12-05T13:53:46.040589Z","shell.execute_reply":"2023-12-05T13:53:47.882343Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 128\nepochs = 200\nlatent_dim = 256\n\nnum_enc_tokens = len(char_set)\nnum_dec_tokens = len(char_set) + 2 # includes \\n \\t\nencoder_inputs = Input(shape=(None,num_enc_tokens))\nencoder = LSTM(latent_dim,return_state=True)\nencoder_outputs , state_h, state_c = encoder(encoder_inputs)\nencoder_states = [state_h,state_c]\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:47.884520Z","iopub.execute_input":"2023-12-05T13:53:47.884797Z","iopub.status.idle":"2023-12-05T13:53:48.232000Z","shell.execute_reply.started":"2023-12-05T13:53:47.884747Z","shell.execute_reply":"2023-12-05T13:53:48.231371Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"decoder_inputs = Input(shape=(None,num_dec_tokens))\ndecoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True)\ndecoder_ouputs,_,_ = decoder_lstm(decoder_inputs,initial_state = encoder_states)\n\ndecoder_dense = Dense(num_dec_tokens, activation='softmax')\ndecoder_ouputs = decoder_dense(decoder_ouputs)\n\nmodel = Model([encoder_inputs,decoder_inputs],decoder_ouputs)\nmodel.compile(optimizer='rmsprop',loss='categorical_crossentropy')\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:48.233237Z","iopub.execute_input":"2023-12-05T13:53:48.233462Z","iopub.status.idle":"2023-12-05T13:53:48.521739Z","shell.execute_reply.started":"2023-12-05T13:53:48.233425Z","shell.execute_reply":"2023-12-05T13:53:48.520835Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, None, 42)     0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            (None, None, 44)     0                                            \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   [(None, 256), (None, 306176      input_1[0][0]                    \n__________________________________________________________________________________________________\nlstm_2 (LSTM)                   [(None, None, 256),  308224      input_2[0][0]                    \n                                                                 lstm_1[0][1]                     \n                                                                 lstm_1[0][2]                     \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, None, 44)     11308       lstm_2[0][0]                     \n==================================================================================================\nTotal params: 625,708\nTrainable params: 625,708\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"h=model.fit([encoder_input_data,decoder_input_data],decoder_target_data\n         ,epochs = epochs,\n          batch_size = batch_size,\n          validation_split = 0.2\n         )\nmodel.save('s2s.h5')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T13:53:48.523765Z","iopub.execute_input":"2023-12-05T13:53:48.524088Z","iopub.status.idle":"2023-12-05T17:14:04.234457Z","shell.execute_reply.started":"2023-12-05T13:53:48.524033Z","shell.execute_reply":"2023-12-05T17:14:04.233663Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Train on 93329 samples, validate on 23333 samples\nEpoch 1/200\n93329/93329 [==============================] - 65s 698us/step - loss: 0.5467 - val_loss: 0.5119\nEpoch 2/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.4379 - val_loss: 0.4144\nEpoch 3/200\n93329/93329 [==============================] - 60s 645us/step - loss: 0.3478 - val_loss: 0.3044\nEpoch 4/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.2643 - val_loss: 0.2537\nEpoch 5/200\n93329/93329 [==============================] - 60s 644us/step - loss: 0.1814 - val_loss: 0.2005\nEpoch 6/200\n93329/93329 [==============================] - 60s 642us/step - loss: 0.1291 - val_loss: 0.1375\nEpoch 7/200\n93329/93329 [==============================] - 60s 644us/step - loss: 0.1010 - val_loss: 0.1146\nEpoch 8/200\n93329/93329 [==============================] - 60s 644us/step - loss: 0.0843 - val_loss: 0.0971\nEpoch 9/200\n93329/93329 [==============================] - 61s 651us/step - loss: 0.0741 - val_loss: 0.0981\nEpoch 10/200\n93329/93329 [==============================] - 61s 652us/step - loss: 0.0668 - val_loss: 0.0796\nEpoch 11/200\n93329/93329 [==============================] - 60s 643us/step - loss: 0.0614 - val_loss: 0.0804\nEpoch 12/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0569 - val_loss: 0.0737\nEpoch 13/200\n93329/93329 [==============================] - 60s 643us/step - loss: 0.0530 - val_loss: 0.0701\nEpoch 14/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0498 - val_loss: 0.0670\nEpoch 15/200\n93329/93329 [==============================] - 60s 645us/step - loss: 0.0468 - val_loss: 0.0696\nEpoch 16/200\n93329/93329 [==============================] - 61s 651us/step - loss: 0.0443 - val_loss: 0.0692\nEpoch 17/200\n93329/93329 [==============================] - 61s 649us/step - loss: 0.0417 - val_loss: 0.0649\nEpoch 18/200\n93329/93329 [==============================] - 61s 650us/step - loss: 0.0394 - val_loss: 0.0669\nEpoch 19/200\n93329/93329 [==============================] - 60s 648us/step - loss: 0.0372 - val_loss: 0.0669\nEpoch 20/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0351 - val_loss: 0.0661\nEpoch 21/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0332 - val_loss: 0.0652\nEpoch 22/200\n93329/93329 [==============================] - 61s 650us/step - loss: 0.0314 - val_loss: 0.0690\nEpoch 23/200\n93329/93329 [==============================] - 61s 649us/step - loss: 0.0295 - val_loss: 0.0695\nEpoch 24/200\n93329/93329 [==============================] - 61s 648us/step - loss: 0.0278 - val_loss: 0.0703\nEpoch 25/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0261 - val_loss: 0.0707\nEpoch 26/200\n93329/93329 [==============================] - 60s 648us/step - loss: 0.0245 - val_loss: 0.0728\nEpoch 27/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0230 - val_loss: 0.0738\nEpoch 28/200\n93329/93329 [==============================] - 61s 648us/step - loss: 0.0216 - val_loss: 0.0764\nEpoch 29/200\n93329/93329 [==============================] - 60s 648us/step - loss: 0.0201 - val_loss: 0.0786\nEpoch 30/200\n93329/93329 [==============================] - 61s 651us/step - loss: 0.0190 - val_loss: 0.0791\nEpoch 31/200\n93329/93329 [==============================] - 61s 649us/step - loss: 0.0177 - val_loss: 0.0829\nEpoch 32/200\n93329/93329 [==============================] - 61s 652us/step - loss: 0.0166 - val_loss: 0.0834\nEpoch 33/200\n93329/93329 [==============================] - 61s 648us/step - loss: 0.0155 - val_loss: 0.0843\nEpoch 34/200\n93329/93329 [==============================] - 61s 648us/step - loss: 0.0146 - val_loss: 0.0857\nEpoch 35/200\n93329/93329 [==============================] - 61s 650us/step - loss: 0.0137 - val_loss: 0.0871\nEpoch 36/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0128 - val_loss: 0.0880\nEpoch 37/200\n93329/93329 [==============================] - 60s 648us/step - loss: 0.0120 - val_loss: 0.0922\nEpoch 38/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0113 - val_loss: 0.0927\nEpoch 39/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0106 - val_loss: 0.0942\nEpoch 40/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0101 - val_loss: 0.0970\nEpoch 41/200\n93329/93329 [==============================] - 61s 649us/step - loss: 0.0095 - val_loss: 0.0966\nEpoch 42/200\n93329/93329 [==============================] - 60s 648us/step - loss: 0.0090 - val_loss: 0.0995\nEpoch 43/200\n93329/93329 [==============================] - 60s 648us/step - loss: 0.0086 - val_loss: 0.1014\nEpoch 44/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0081 - val_loss: 0.1027\nEpoch 45/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0077 - val_loss: 0.1016\nEpoch 46/200\n93329/93329 [==============================] - 60s 648us/step - loss: 0.0074 - val_loss: 0.1023\nEpoch 47/200\n93329/93329 [==============================] - 61s 649us/step - loss: 0.0071 - val_loss: 0.1029\nEpoch 48/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0068 - val_loss: 0.1055\nEpoch 49/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0066 - val_loss: 0.1047\nEpoch 50/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0063 - val_loss: 0.1082\nEpoch 51/200\n93329/93329 [==============================] - 60s 645us/step - loss: 0.0061 - val_loss: 0.1076\nEpoch 52/200\n93329/93329 [==============================] - 61s 649us/step - loss: 0.0059 - val_loss: 0.1072\nEpoch 53/200\n93329/93329 [==============================] - 61s 650us/step - loss: 0.0058 - val_loss: 0.1109\nEpoch 54/200\n93329/93329 [==============================] - 60s 645us/step - loss: 0.0055 - val_loss: 0.1106\nEpoch 55/200\n93329/93329 [==============================] - 60s 648us/step - loss: 0.0053 - val_loss: 0.1118\nEpoch 56/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0052 - val_loss: 0.1107\nEpoch 57/200\n93329/93329 [==============================] - 61s 649us/step - loss: 0.0051 - val_loss: 0.1109\nEpoch 58/200\n93329/93329 [==============================] - 60s 648us/step - loss: 0.0050 - val_loss: 0.1172\nEpoch 59/200\n93329/93329 [==============================] - 60s 645us/step - loss: 0.0049 - val_loss: 0.1117\nEpoch 60/200\n93329/93329 [==============================] - 60s 648us/step - loss: 0.0047 - val_loss: 0.1147\nEpoch 61/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0046 - val_loss: 0.1141\nEpoch 62/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0045 - val_loss: 0.1156\nEpoch 63/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0044 - val_loss: 0.1158\nEpoch 64/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0044 - val_loss: 0.1151\nEpoch 65/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0043 - val_loss: 0.1182\nEpoch 66/200\n93329/93329 [==============================] - 60s 648us/step - loss: 0.0042 - val_loss: 0.1152\nEpoch 67/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0041 - val_loss: 0.1170\nEpoch 68/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0041 - val_loss: 0.1184\nEpoch 69/200\n93329/93329 [==============================] - 61s 649us/step - loss: 0.0040 - val_loss: 0.1164\nEpoch 70/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0039 - val_loss: 0.1163\nEpoch 71/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0039 - val_loss: 0.1194\nEpoch 72/200\n93329/93329 [==============================] - 60s 638us/step - loss: 0.0038 - val_loss: 0.1189\nEpoch 73/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0038 - val_loss: 0.1189\nEpoch 74/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0037 - val_loss: 0.1194\nEpoch 75/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0037 - val_loss: 0.1199\nEpoch 76/200\n93329/93329 [==============================] - 60s 643us/step - loss: 0.0036 - val_loss: 0.1191\nEpoch 77/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0036 - val_loss: 0.1229\nEpoch 78/200\n93329/93329 [==============================] - 60s 638us/step - loss: 0.0035 - val_loss: 0.1214\nEpoch 79/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0034 - val_loss: 0.1232\nEpoch 80/200\n93329/93329 [==============================] - 60s 638us/step - loss: 0.0035 - val_loss: 0.1202\nEpoch 81/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0033 - val_loss: 0.1216\nEpoch 82/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0034 - val_loss: 0.1230\nEpoch 83/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0033 - val_loss: 0.1231\nEpoch 84/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0033 - val_loss: 0.1233\nEpoch 85/200\n93329/93329 [==============================] - 60s 642us/step - loss: 0.0032 - val_loss: 0.1234\nEpoch 86/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0032 - val_loss: 0.1233\nEpoch 87/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0032 - val_loss: 0.1246\nEpoch 88/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0032 - val_loss: 0.1234\nEpoch 89/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0031 - val_loss: 0.1237\nEpoch 90/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0030 - val_loss: 0.1259\nEpoch 91/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0031 - val_loss: 0.1266\nEpoch 92/200\n93329/93329 [==============================] - 60s 642us/step - loss: 0.0030 - val_loss: 0.1253\nEpoch 93/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0030 - val_loss: 0.1266\nEpoch 94/200\n93329/93329 [==============================] - 60s 638us/step - loss: 0.0030 - val_loss: 0.1268\nEpoch 95/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0030 - val_loss: 0.1270\nEpoch 96/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0029 - val_loss: 0.1255\nEpoch 97/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0029 - val_loss: 0.1287\nEpoch 98/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0029 - val_loss: 0.1269\nEpoch 99/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0028 - val_loss: 0.1267\nEpoch 100/200\n93329/93329 [==============================] - 59s 637us/step - loss: 0.0028 - val_loss: 0.1271\nEpoch 101/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0028 - val_loss: 0.1281\nEpoch 102/200\n93329/93329 [==============================] - 60s 642us/step - loss: 0.0027 - val_loss: 0.1291\nEpoch 103/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0027 - val_loss: 0.1276\nEpoch 104/200\n93329/93329 [==============================] - 60s 642us/step - loss: 0.0027 - val_loss: 0.1281\nEpoch 105/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0027 - val_loss: 0.1297\nEpoch 106/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0027 - val_loss: 0.1295\nEpoch 107/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0027 - val_loss: 0.1338\nEpoch 108/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0026 - val_loss: 0.1289\nEpoch 109/200\n93329/93329 [==============================] - 59s 637us/step - loss: 0.0026 - val_loss: 0.1292\nEpoch 110/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0026 - val_loss: 0.1284\nEpoch 111/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0026 - val_loss: 0.1298\nEpoch 112/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0025 - val_loss: 0.1293\nEpoch 113/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0025 - val_loss: 0.1313\nEpoch 114/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0025 - val_loss: 0.1307\nEpoch 115/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0025 - val_loss: 0.1289\nEpoch 116/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0025 - val_loss: 0.1289\nEpoch 117/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0024 - val_loss: 0.1319\nEpoch 118/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0025 - val_loss: 0.1284\nEpoch 119/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0024 - val_loss: 0.1314\nEpoch 120/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0024 - val_loss: 0.1326\nEpoch 121/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0024 - val_loss: 0.1305\nEpoch 122/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0024 - val_loss: 0.1299\nEpoch 123/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0024 - val_loss: 0.1299\nEpoch 124/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0024 - val_loss: 0.1313\nEpoch 125/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0023 - val_loss: 0.1308\nEpoch 126/200\n93329/93329 [==============================] - 59s 637us/step - loss: 0.0023 - val_loss: 0.1301\nEpoch 127/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0023 - val_loss: 0.1344\nEpoch 128/200\n93329/93329 [==============================] - 60s 642us/step - loss: 0.0024 - val_loss: 0.1311\nEpoch 129/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0023 - val_loss: 0.1313\nEpoch 130/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0023 - val_loss: 0.1321\nEpoch 131/200\n93329/93329 [==============================] - 60s 643us/step - loss: 0.0023 - val_loss: 0.1325\nEpoch 132/200\n93329/93329 [==============================] - 60s 642us/step - loss: 0.0023 - val_loss: 0.1321\nEpoch 133/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0022 - val_loss: 0.1311\nEpoch 134/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0022 - val_loss: 0.1343\nEpoch 135/200\n93329/93329 [==============================] - 60s 638us/step - loss: 0.0022 - val_loss: 0.1325\nEpoch 136/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0022 - val_loss: 0.1328\nEpoch 137/200\n93329/93329 [==============================] - 60s 643us/step - loss: 0.0022 - val_loss: 0.1333\nEpoch 138/200\n93329/93329 [==============================] - 60s 643us/step - loss: 0.0022 - val_loss: 0.1336\nEpoch 139/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0022 - val_loss: 0.1314\nEpoch 140/200\n93329/93329 [==============================] - 60s 643us/step - loss: 0.0022 - val_loss: 0.1329\nEpoch 141/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0022 - val_loss: 0.1342\nEpoch 142/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0021 - val_loss: 0.1341\nEpoch 143/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0021 - val_loss: 0.1339\nEpoch 144/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0021 - val_loss: 0.1325\nEpoch 145/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0022 - val_loss: 0.1333\nEpoch 146/200\n93329/93329 [==============================] - 60s 638us/step - loss: 0.0021 - val_loss: 0.1341\nEpoch 147/200\n93329/93329 [==============================] - 61s 654us/step - loss: 0.0021 - val_loss: 0.1339\nEpoch 148/200\n93329/93329 [==============================] - 62s 660us/step - loss: 0.0021 - val_loss: 0.1324\nEpoch 149/200\n93329/93329 [==============================] - 60s 643us/step - loss: 0.0021 - val_loss: 0.1335\nEpoch 150/200\n93329/93329 [==============================] - 60s 642us/step - loss: 0.0020 - val_loss: 0.1341\nEpoch 151/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0020 - val_loss: 0.1359\nEpoch 152/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0021 - val_loss: 0.1332\nEpoch 153/200\n93329/93329 [==============================] - 60s 638us/step - loss: 0.0020 - val_loss: 0.1357\nEpoch 154/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0020 - val_loss: 0.1356\nEpoch 155/200\n93329/93329 [==============================] - 59s 637us/step - loss: 0.0020 - val_loss: 0.1331\nEpoch 156/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0020 - val_loss: 0.1341\nEpoch 157/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0019 - val_loss: 0.1345\nEpoch 158/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0019 - val_loss: 0.1346\nEpoch 159/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0019 - val_loss: 0.1351\nEpoch 160/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0019 - val_loss: 0.1352\nEpoch 161/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0019 - val_loss: 0.1353\nEpoch 162/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0019 - val_loss: 0.1357\nEpoch 163/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0019 - val_loss: 0.1346\nEpoch 164/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0019 - val_loss: 0.1351\nEpoch 165/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0019 - val_loss: 0.1352\nEpoch 166/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0019 - val_loss: 0.1352\nEpoch 167/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0019 - val_loss: 0.1364\nEpoch 168/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0019 - val_loss: 0.1362\nEpoch 169/200\n93329/93329 [==============================] - 60s 642us/step - loss: 0.0019 - val_loss: 0.1349\nEpoch 170/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0019 - val_loss: 0.1365\nEpoch 171/200\n93329/93329 [==============================] - 60s 639us/step - loss: 0.0019 - val_loss: 0.1364\nEpoch 172/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0018 - val_loss: 0.1353\nEpoch 173/200\n93329/93329 [==============================] - 60s 638us/step - loss: 0.0019 - val_loss: 0.1353\nEpoch 174/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0018 - val_loss: 0.1360\nEpoch 175/200\n93329/93329 [==============================] - 60s 648us/step - loss: 0.0019 - val_loss: 0.1376\nEpoch 176/200\n93329/93329 [==============================] - 60s 645us/step - loss: 0.0018 - val_loss: 0.1354\nEpoch 177/200\n93329/93329 [==============================] - 60s 644us/step - loss: 0.0018 - val_loss: 0.1363\nEpoch 178/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0017 - val_loss: 0.1358\nEpoch 179/200\n93329/93329 [==============================] - 60s 648us/step - loss: 0.0018 - val_loss: 0.1360\nEpoch 180/200\n93329/93329 [==============================] - 61s 654us/step - loss: 0.0018 - val_loss: 0.1357\nEpoch 181/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0018 - val_loss: 0.1349\nEpoch 182/200\n93329/93329 [==============================] - 60s 642us/step - loss: 0.0018 - val_loss: 0.1369\nEpoch 183/200\n93329/93329 [==============================] - 60s 640us/step - loss: 0.0018 - val_loss: 0.1363\nEpoch 184/200\n93329/93329 [==============================] - 60s 642us/step - loss: 0.0018 - val_loss: 0.1364\nEpoch 185/200\n93329/93329 [==============================] - 61s 652us/step - loss: 0.0018 - val_loss: 0.1374\nEpoch 186/200\n93329/93329 [==============================] - 60s 647us/step - loss: 0.0017 - val_loss: 0.1369\nEpoch 187/200\n93329/93329 [==============================] - 60s 646us/step - loss: 0.0018 - val_loss: 0.1369\nEpoch 188/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0017 - val_loss: 0.1373\nEpoch 189/200\n93329/93329 [==============================] - 60s 644us/step - loss: 0.0017 - val_loss: 0.1363\nEpoch 190/200\n93329/93329 [==============================] - 61s 650us/step - loss: 0.0017 - val_loss: 0.1370\nEpoch 191/200\n93329/93329 [==============================] - 61s 650us/step - loss: 0.0017 - val_loss: 0.1370\nEpoch 192/200\n93329/93329 [==============================] - 61s 649us/step - loss: 0.0017 - val_loss: 0.1370\nEpoch 193/200\n93329/93329 [==============================] - 60s 644us/step - loss: 0.0017 - val_loss: 0.1358\nEpoch 194/200\n93329/93329 [==============================] - 60s 645us/step - loss: 0.0016 - val_loss: 0.1367\nEpoch 195/200\n93329/93329 [==============================] - 60s 642us/step - loss: 0.0017 - val_loss: 0.1368\nEpoch 196/200\n93329/93329 [==============================] - 60s 645us/step - loss: 0.0017 - val_loss: 0.1385\nEpoch 197/200\n93329/93329 [==============================] - 60s 643us/step - loss: 0.0017 - val_loss: 0.1375\nEpoch 198/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0016 - val_loss: 0.1370\nEpoch 199/200\n93329/93329 [==============================] - 60s 642us/step - loss: 0.0016 - val_loss: 0.1372\nEpoch 200/200\n93329/93329 [==============================] - 60s 641us/step - loss: 0.0016 - val_loss: 0.1375\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n  '. They will not be included '\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(h.history['loss'])\nplt.title('Model Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:14:04.236218Z","iopub.execute_input":"2023-12-05T17:14:04.236554Z","iopub.status.idle":"2023-12-05T17:14:04.394868Z","shell.execute_reply.started":"2023-12-05T17:14:04.236487Z","shell.execute_reply":"2023-12-05T17:14:04.394200Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"encoder_model = Model(encoder_inputs,encoder_states)\n\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\ndecoder_outputs,state_h,state_c = decoder_lstm(\n        decoder_inputs,initial_state = decoder_states_inputs\n)\ndecoder_states = [state_h,state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states\n)\nencoder_model.save('encoder.h5')\ndecoder_model.save('decoder.h5')\n\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_dec_tokens))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0, char2int['\\t']] = 1.\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = int2char[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '\\n' or\n           len(decoded_sentence) > max_dec_len):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_dec_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence\n\nsample_text=\"\"\ncorrected_text=\"\"\nresult_text=\"\"\n\nfor seq_index in range(107):\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    wrong_word=input_texts[seq_index]+\" \"\n    corrected_word=decoded_sentence+\" \"\n    ground_word=target_texts[seq_index]+\" \"\n    sample_text+=wrong_word\n    corrected_text+=corrected_word\n    result_text+=ground_word\n    #print('-')\n    #rint('Wrong sentence:', input_texts[seq_index])\n    #print('Corrected sentence:', decoded_sentence)\n    #print('Ground Truth:',target_texts[seq_index])\n    \n#print(sample_text)\n#print(corrected_text)\n#print(result_text)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:14:04.396225Z","iopub.execute_input":"2023-12-05T17:14:04.396447Z","iopub.status.idle":"2023-12-05T17:14:07.039634Z","shell.execute_reply.started":"2023-12-05T17:14:04.396410Z","shell.execute_reply":"2023-12-05T17:14:07.038984Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_3:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_4:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n  '. They will not be included '\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluation(sample_text, corrected_text, result_text):\n    true_labels = nltk.word_tokenize(corrected_text)\n    \n    result_predicted = nltk.word_tokenize(result_text)\n    wer = Levenshtein.distance(' '.join(true_labels), ' '.join(result_predicted)) / len(true_labels)\n    cer = Levenshtein.distance(''.join(true_labels), ''.join(result_predicted)) / len(''.join(true_labels))\n    print(f\"Word Error Rate (WER): {wer:.2%}\")\n    print(f\"Character Error Rate (CER): {cer:.2%}\")\n\nevaluation(sample_text,corrected_text,result_text)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:14:07.040981Z","iopub.execute_input":"2023-12-05T17:14:07.041213Z","iopub.status.idle":"2023-12-05T17:14:07.063734Z","shell.execute_reply.started":"2023-12-05T17:14:07.041174Z","shell.execute_reply":"2023-12-05T17:14:07.062910Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Word Error Rate (WER): 0.93%\nCharacter Error Rate (CER): 0.12%\n","output_type":"stream"}]},{"cell_type":"code","source":"# trials","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:14:07.065086Z","iopub.execute_input":"2023-12-05T17:14:07.065399Z","iopub.status.idle":"2023-12-05T17:14:07.068969Z","shell.execute_reply.started":"2023-12-05T17:14:07.065342Z","shell.execute_reply":"2023-12-05T17:14:07.068186Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# encoder_input_data\n# input_seq\n# decoded_sentance\n# input_texts and target_texts from lines","metadata":{"execution":{"iopub.status.busy":"2023-12-05T17:14:07.070296Z","iopub.execute_input":"2023-12-05T17:14:07.070575Z","iopub.status.idle":"2023-12-05T17:14:07.078663Z","shell.execute_reply.started":"2023-12-05T17:14:07.070520Z","shell.execute_reply":"2023-12-05T17:14:07.077924Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}